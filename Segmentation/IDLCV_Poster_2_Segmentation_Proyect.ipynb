{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation Exercise POSTER 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this exercise stems from /dtu/datasets1/0251\n",
    "The data loader below assumes that you are working on the HPC machines with the data located at  \n",
    "\n",
    "/dtu/datasets1/02516/phc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "\n",
    "# pip install torchsummary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/dtu/datasets1/02516/phc_data'\n",
    "#data_path = './phc_data'\n",
    "class DataEyeLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, train, transform, data_path=data_path):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        data_path = os.path.join(data_path, 'train' if train else 'test')\n",
    "        self.image_paths = sorted(glob.glob(data_path + '/images/*.jpg'))\n",
    "        self.label_paths = sorted(glob.glob(data_path + '/labels/*.png'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        label = Image.open(label_path)\n",
    "        Y = self.transform(label)\n",
    "        X = self.transform(image)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/dtu/datasets1/02516/phc_data'\n",
    "#data_path = './phc_data'\n",
    "class DataSkinLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, train, transform, data_path=data_path):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        data_path = os.path.join(data_path, 'train' if train else 'test')\n",
    "        self.image_paths = sorted(glob.glob(data_path + '/images/*.jpg'))\n",
    "        self.label_paths = sorted(glob.glob(data_path + '/labels/*.png'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        label = Image.open(label_path)\n",
    "        Y = self.transform(label)\n",
    "        X = self.tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the images could have different sizes. Let's resize them all to $128\\times128$ pixels, using the torchvision Resize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size = 128\n",
    "train_transform = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.Resize((size, size)), \n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "batch_size = 6\n",
    "trainset = DataEyeLoader(train=True, transform=train_transform)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "testset = PhC(train=False, transform=test_transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loaded %d training images' % len(trainset))\n",
    "print('Loaded %d test images' % len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some images from the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [18, 6]\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 6, i+1)\n",
    "    plt.imshow(np.swapaxes(np.swapaxes(images[i], 0, 2), 0, 1))\n",
    "\n",
    "    plt.subplot(2, 6, i+7)\n",
    "    plt.imshow(labels[i].squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device\n",
    "\n",
    "Check if GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider a simple encoder decoder network for image registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder (downsampling)\n",
    "        self.enc_conv0 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.pool0 = nn.MaxPool2d(2, 2)  # 128 -> 64\n",
    "        self.enc_conv1 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 64 -> 32\n",
    "        self.enc_conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 32 -> 16\n",
    "        self.enc_conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 16 -> 8\n",
    "\n",
    "        # bottleneck\n",
    "        self.bottleneck_conv = nn.Conv2d(64, 64, 3, padding=1)\n",
    "\n",
    "        # decoder (upsampling)\n",
    "        self.upsample0 = nn.Upsample(16)  # 8 -> 16\n",
    "        self.dec_conv0 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.upsample1 = nn.Upsample(32)  # 16 -> 32\n",
    "        self.dec_conv1 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.upsample2 = nn.Upsample(64)  # 32 -> 64\n",
    "        self.dec_conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.upsample3 = nn.Upsample(128)  # 64 -> 128\n",
    "        self.dec_conv3 = nn.Conv2d(64, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        e0 = self.pool0(F.relu(self.enc_conv0(x)))\n",
    "        e1 = self.pool1(F.relu(self.enc_conv1(e0)))\n",
    "        e2 = self.pool2(F.relu(self.enc_conv2(e1)))\n",
    "        e3 = self.pool3(F.relu(self.enc_conv3(e2)))\n",
    "\n",
    "        # bottleneck\n",
    "        b = F.relu(self.bottleneck_conv(e3))\n",
    "\n",
    "        # decoder\n",
    "        d0 = F.relu(self.dec_conv0(self.upsample0(b)))\n",
    "        d1 = F.relu(self.dec_conv1(self.upsample1(d0)))\n",
    "        d2 = F.relu(self.dec_conv2(self.upsample2(d1)))\n",
    "        d3 = self.dec_conv3(self.upsample3(d2))  # no activation\n",
    "        return d3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the following metrics for validating segmentation performance:  Dice overlap, Intersection overUnion, Accuracy, Sensitivity, and Specificy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice overalp\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-6  # To avoid division by zero\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "\n",
    "def dice_coefficient(y_pred, y_true, epsilon=1e-07):\n",
    "    y_pred_copy = prediction.clone()\n",
    "\n",
    "    y_pred_copy[prediction_copy < 0] = 0\n",
    "    y_pred_copy[prediction_copy > 0] = 1\n",
    "\n",
    "    intersection = abs(torch.sum(y_pred_copy * y_true))\n",
    "    union = abs(torch.sum(y_pred_copy) + torch.sum(y_true))\n",
    "    dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "    return dice\n",
    "\n",
    "# Intersection overUnion\n",
    "\n",
    "def intersection_over_union(y_true, y_pred):\n",
    "    smooth = 1e-6  # To avoid division by zero\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    union = y_true_f.sum() + y_pred_f.sum() - intersection\n",
    "    return intersection / (union + smooth)\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_pred = y_pred.round()  # Convert probabilities to binary\n",
    "    correct = (y_true == y_pred).float()  # Check if predictions are correct\n",
    "    return correct.sum() / correct.numel()\n",
    "\n",
    "# Sensitivity (measures true positives)\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    y_pred = y_pred.round()  # Convert probabilities to binary\n",
    "    true_positives = (y_true * y_pred).sum()\n",
    "    possible_positives = y_true.sum()\n",
    "    return true_positives / (possible_positives + 1e-6)  # Avoid division by zero\n",
    "\n",
    "# Specificity(measures true negatives)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    y_pred = y_pred.round()  # Convert probabilities to binary\n",
    "    true_negatives = ((1 - y_true) * (1 - y_pred)).sum()\n",
    "    possible_negatives = (1 - y_true).sum()\n",
    "    return true_negatives / (possible_negatives + 1e-6)  # Avoid division by zero\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_segmentation_metrics(y_true, y_pred, threshold=0.5):\n",
    "    y_pred = y_pred.round()\n",
    "    TP = ((y_true == 1) & (y_pred == 1)).sum().item()\n",
    "    FP = ((y_true == 0) & (y_pred == 1)).sum().item()\n",
    "    TN = ((y_true == 0) & (y_pred == 0)).sum().item()\n",
    "    FN = ((y_true == 1) & (y_pred == 0)).sum().item()\n",
    "    # Calcular Dice\n",
    "    dice = (2 * TP) / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0\n",
    "\n",
    "    # Calcular IoU\n",
    "    iou = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
    "\n",
    "    # Calcular Accuracy\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "\n",
    "    # Calcular Sensibilidad\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    # Calcular Especificidad\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        'Dice': dice,\n",
    "        'IoU': iou,\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryJaccardIndex, BinaryRecall, BinarySpecificity, BinaryDiceCoefficient\n",
    "\n",
    "def calculate_segmentation_metrics(y_true,y_pred):\n",
    "y_pred = y_pred.round()\n",
    "\n",
    "dice_score2=Dice(y_pred, y_true)\n",
    "\n",
    "dice_score  = BinaryDiceCoefficient(y_pred,y_true)\n",
    "iou_score = BinaryJaccardIndex(y_pred,y_true)\n",
    "accuracy_score = BinaryAccuracy(y_pred,y_true)\n",
    "sensitivity_score=BinaryRecall(y_pred,y_true)\n",
    "specificity_score=BinarySpecificity(y_pred,y_true)\n",
    "\n",
    "metrics = {\n",
    "        'Dice': dice,\n",
    "        'IoU': iou,\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity\n",
    "    }\n",
    "return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement  a  U-net  architecture  to  segment  the  image. We will use Convolutions with stride 2 for downsampling and ConvTranspose for upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.enc2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.enc3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.dec2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dec3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.out_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "        # Convoluciones para downsampling y upsampling\n",
    "        self.downsample1 = nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2)  # Reemplazado\n",
    "        self.downsample2 = nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=2)  # Reemplazado\n",
    "        self.downsample3 = nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=2)  # Reemplazado\n",
    "        self.upsample1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)  # Usando transposed convolutions\n",
    "        self.upsample2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # Usando transposed convolutions\n",
    "        self.upsample3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)  # Usando transposed convolutions\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = F.relu(self.enc1(x))\n",
    "        e2 = F.relu(self.downsample1(e1))  # Usamos convolución con stride=2\n",
    "        e2 = F.relu(self.enc2(e2))\n",
    "        e3 = F.relu(self.downsample2(e2))  # Usamos convolución con stride=2\n",
    "        e3 = F.relu(self.enc3(e3))\n",
    "        b = F.relu(self.bottleneck(e3))\n",
    "\n",
    "        # Decoder\n",
    "        d1 = self.upsample1(b)\n",
    "        d1 = torch.cat((d1, e3), dim=1)  # Skip connection\n",
    "        d1 = F.relu(self.dec1(d1))\n",
    "        d2 = self.upsample2(d1)\n",
    "        d2 = torch.cat((d2, e2), dim=1)  # Skip connection\n",
    "        d2 = F.relu(self.dec2(d2))\n",
    "        d3 = self.upsample3(d2)\n",
    "        d3 = torch.cat((d3, e1), dim=1)  # Skip connection\n",
    "        d3 = F.relu(self.dec3(d3))\n",
    "\n",
    "        return self.out_conv(d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_real, y_pred):\n",
    "    return torch.mean(y_pred - y_real*y_pred + torch.log(1 + torch.exp(-y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_real, y_pred):\n",
    "    return 1 - ((2*y_real*y_pred + 1).mean())/((y_real+y_pred).mean()+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncDec().to(device)\n",
    "train(model, optim.Adam(model.parameters(), 0.0001), dice_loss, 20, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 2\n",
    "def focal_loss(y_real, y_pred,gamma):\n",
    "    y_pred_sig = torch.sigmoid(y_pred)\n",
    "    term = (1-y_pred_sig)**gamma * y_real * torch.log(y_pred_sig) + (1-y_real)torch.log(1-y_pred_sig)\n",
    "    return (-term.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncDec().to(device)\n",
    "train(model, optim.Adam(model.parameters(), 0.0001), focal_loss, 20, train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
