{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "def generate_clicks(mask, num_positives, num_negatives, margin, var):\n",
    "\n",
    "    # Get object (foregorund) and background pixels\n",
    "    object_pixels = np.argwhere(mask==True)\n",
    "    lesion_dilated = binary_dilation(mask, iterations=margin)  # Dilate lesion area by margin\n",
    "    background_pixels = np.argwhere((mask == False) & (~lesion_dilated))\n",
    "    \n",
    "    # Sample positive clicks from object pixels\n",
    "    positive_clicks=[] # List of random positions for positive clicks\n",
    "    while len(positive_clicks) < num_positives:\n",
    "        new_pos_click = random.choice(object_pixels)\n",
    "        positive_clicks.append(new_pos_click)\n",
    "\n",
    "    # Sample negative clicks from background pixels\n",
    "    negative_clicks=[]\n",
    "    while len(negative_clicks) < num_negatives:\n",
    "        new_neg_click= random.choice(background_pixels)\n",
    "        negative_clicks.append(new_neg_click)\n",
    "\n",
    "    return positive_clicks, negative_clicks\n",
    "\n",
    "# Define the function to draw the ramdom points in the mask\n",
    "\n",
    "# def draw_clicks_on_original(image_path, positive_clicks, negative_clicks, circle_radius,border_thickness):\n",
    "#     # Create a copy of the mask to draw on\n",
    "#     original_image = Image.open(image_path).convert('RGB')  \n",
    "#     draw = ImageDraw.Draw(original_image)\n",
    "\n",
    "#     # Draw positive clicks (in green)\n",
    "#     for (_,y,x) in positive_clicks:\n",
    "#         # Draw outer circle for black border\n",
    "#         draw.ellipse((x - circle_radius - border_thickness, y - circle_radius - border_thickness,\n",
    "#                       x + circle_radius + border_thickness, y + circle_radius + border_thickness), fill=\"black\")\n",
    "#         # Draw inner green circle\n",
    "#         draw.ellipse((x- circle_radius, y-circle_radius, x + circle_radius, y+ circle_radius), outline='green', fill='green') \n",
    "\n",
    "#     for (_, y,x) in negative_clicks:\n",
    "#         # Draw outer circle for black border\n",
    "#         draw.ellipse((x - circle_radius - border_thickness, y - circle_radius - border_thickness,\n",
    "#                       x + circle_radius + border_thickness, y + circle_radius + border_thickness), fill=\"black\")\n",
    "#         # Draw inner red circle\n",
    "#         draw.ellipse((x-circle_radius, y-circle_radius, x + circle_radius, y + circle_radius), outline='red', fill='red')\n",
    "\n",
    "#     return original_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def generate_clustered_annotations(mask, num_clusters_pos, num_clusters_neg, margin):\n",
    "    # Obtener índices de la lesión y el fondo\n",
    "    lesion_pixels = np.argwhere(mask == True)\n",
    "    background_pixels = np.argwhere(mask == False)\n",
    "    \n",
    "    # Aplicar K-Means para los píxeles de la lesión\n",
    "    kmeans = KMeans(n_clusters=num_clusters_pos, random_state=0).fit(lesion_pixels)\n",
    "    positive_clicks = kmeans.cluster_centers_.astype(int)  # Centroides de los clusters\n",
    "    \n",
    "    # Filtrar los índices de fondo con un margen\n",
    "    from scipy.ndimage import binary_dilation\n",
    "    lesion_dilated = binary_dilation(mask, iterations=margin)\n",
    "    background_pixels_margin = np.argwhere((mask == 0) & (~lesion_dilated))\n",
    "    \n",
    "    # Seleccionar puntos negativos aleatorios\n",
    "    kmeans_background = KMeans(n_clusters=num_clusters_neg, random_state=0).fit(background_pixels_margin)\n",
    "    negative_clicks = kmeans_background.cluster_centers_.astype(int) \n",
    "\n",
    "    return positive_clicks, negative_clicks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the Data Loader using ramdom points( We leave it the same way)\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SkinLesionLoader(Dataset):\n",
    "    def __init__(self, transform, dataset_path, num_positives=5, num_negatives=5, margin=10, var=20, circle_radius=5, border_thickness=2, split='train'):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        self.num_positives = num_positives\n",
    "        self.num_negatives = num_negatives\n",
    "        self.margin = margin\n",
    "        self.var = var\n",
    "        self.circle_radius = circle_radius\n",
    "        self.border_thickness = border_thickness\n",
    "        \n",
    "        # Collect all image paths and label paths\n",
    "        self.image_paths = []\n",
    "        self.label_paths = []\n",
    "\n",
    "        # Loop through all IMG### folders\n",
    "        for folder_name in os.listdir(dataset_path):\n",
    "            folder_path = os.path.join(dataset_path, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                # Add dermoscopic image path\n",
    "                dermoscopic_image_path = os.path.join(folder_path, f'{folder_name}_Dermoscopic_Image', f'{folder_name}.bmp')\n",
    "                if os.path.exists(dermoscopic_image_path):\n",
    "                    self.image_paths.append(dermoscopic_image_path)\n",
    "                \n",
    "                # Add lesion image path\n",
    "                lesion_image_path = os.path.join(folder_path, f'{folder_name}_lesion', f'{folder_name}_lesion.bmp')\n",
    "                if os.path.exists(lesion_image_path):\n",
    "                    self.label_paths.append(lesion_image_path)\n",
    "\n",
    "        # Ensure both lists have the same length\n",
    "        assert len(self.image_paths) == len(self.label_paths), \"Image and label counts do not match.\"\n",
    "\n",
    "        # Randomly shuffle the dataset\n",
    "        combined = list(zip(self.image_paths, self.label_paths))\n",
    "        random.shuffle(combined)\n",
    "        self.image_paths, self.label_paths = zip(*combined)\n",
    "\n",
    "        # Split indices for 70-15-15\n",
    "        total_len = len(self.image_paths)\n",
    "        train_split = int(total_len * 0.7)\n",
    "        val_split = int(total_len * 0.85)  # 70% for training, next 15% for validation\n",
    "\n",
    "        if split == 'train':\n",
    "            self.image_paths = self.image_paths[:train_split]\n",
    "            self.label_paths = self.label_paths[:train_split]\n",
    "        elif split == 'val':\n",
    "            self.image_paths = self.image_paths[train_split:val_split]\n",
    "            self.label_paths = self.label_paths[train_split:val_split]\n",
    "        elif split == 'test':\n",
    "            self.image_paths = self.image_paths[val_split:]\n",
    "            self.label_paths = self.label_paths[val_split:]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split name. Use 'train', 'val', or 'test'.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Ensure image is in RGB format\n",
    "        mask = Image.open(label_path).convert(\"L\")    # Ensure label is in grayscale\n",
    "\n",
    "        X = self.transform(image)\n",
    "        Y = self.transform(mask)\n",
    "\n",
    "        # Convert mask to binary format for lesion/background distinction\n",
    "        mask_array = np.array(Y) > 0\n",
    "\n",
    "        # Generate positive and negative clicks using the mask\n",
    "        positive_clicks, negative_clicks = generate_clicks(mask_array, self.num_positives, self.num_negatives, self.margin, self.var)\n",
    "\n",
    "        # Draw clicks on the original image\n",
    "        # annotated_image = draw_clicks_on_original(image_path, positive_clicks, negative_clicks, self.circle_radius, self.border_thickness)\n",
    "        \n",
    "        \n",
    "        return X, positive_clicks, negative_clicks\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_skin_lesion_loader(data_path, batch_size=1):\n",
    "    # Transforms para preprocesamiento\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Redimensiona las imágenes\n",
    "        transforms.ToTensor()           # Convierte a tensor\n",
    "    ])\n",
    "    \n",
    "    # Carga el conjunto de datos con los parámetros necesarios\n",
    "    dataset = SkinLesionLoader(transform=transform, dataset_path=data_path, split='train')\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Obtiene un lote\n",
    "    X_batch, positive_clicks_batch, negative_clicks_batch = next(iter(data_loader))\n",
    "\n",
    "    # Visualiza cada imagen en el lote junto con las anotaciones de los clics\n",
    "    # fig, axes = plt.subplots(2, batch_size, figsize=(15, 6))\n",
    "\n",
    "    # print(X_batch[0].shape)\n",
    "\n",
    "    # for i in range(batch_size):\n",
    "    #     # Muestra la imagen original transformada\n",
    "    #     # axes[0, i].imshow(X_batch[i].permute(1, 2, 0))\n",
    "    #     axes[0, i].imshow(torch.permute(X_batch[i], (1, 2, 0)))\n",
    "    #     axes[0, i].set_title(\"Processed image\")\n",
    "    #     axes[0, i].axis('off')\n",
    "\n",
    "    #     # Muestra la imagen anotada con clics\n",
    "    #     # axes[1, i].imshow(annotated_images[i].permute(1, 2, 0))\n",
    "    #     # axes[1, i].set_title(\"Imagen con Clics\")\n",
    "    #     # axes[1, i].axis('off')\n",
    "\n",
    "    #     # Muestra las coordenadas de clics generadas\n",
    "    #     print(f\"Image {i+1}:\")\n",
    "    #     print(\" Positive clicks:\", positive_clicks_batch[i])\n",
    "    #     print(\" Negative clicks:\", negative_clicks_batch[i])\n",
    "    \n",
    "    # plt.show()\n",
    "\n",
    "test_skin_lesion_loader(\"./dtu/datasets1/02516/PH2_Dataset_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def L_point(output, pos_clicks_position, neg_clicks_position, alpha=0.75):\n",
    "    # We extract the known points for the mask, but for the ouput\n",
    "    positive_preds = torch.stack([output[0, y, x] for (y, x) in pos_clicks_position])\n",
    "    negative_preds = torch.stack([output[0, y, x] for (y, x) in neg_clicks_position])\n",
    "\n",
    "    # We use the BCE Loss\n",
    "    bce_loss= nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Labels: 1 for positive and 0 for negative\n",
    "    positive_labels = torch.ones_like(positive_preds)\n",
    "    negative_labels = torch.zeros_like(negative_preds)\n",
    "\n",
    "    # Calculate the BCE for the positive and negatives\n",
    "    loss_positive = bce_loss(positive_preds, positive_labels)\n",
    "    loss_negative = bce_loss(negative_preds, negative_labels)\n",
    "\n",
    "\n",
    "    # Calculate the curretn loss adding all the losses for the positives and negatives\n",
    "    total_loss = alpha*loss_positive + (1-alpha)*loss_negative\n",
    "    return total_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(output, target, alpha=0.25, gamma=2.0):\n",
    "   \n",
    "    # Convertir logits a probabilidades\n",
    "    probs = torch.sigmoid(output)\n",
    "    \n",
    "    # Calcular la pérdida focal\n",
    "    loss = -alpha * (1 - probs) ** gamma * (target * torch.log(probs + 1e-8) + (1 - target) * torch.log(1 - probs + 1e-8))\n",
    "    \n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the training function accordingly\n",
    "\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_with_validation(device, model, opt, loss_fn, epochs, train_loader, val_loader, test_loader, patience=5):\n",
    "    #X_test, Y_test, num_pos_test, num_neg_test = next(iter(test_loader))\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tic = time()\n",
    "        print('* Epoch %d/%d' % (epoch+1, epochs))\n",
    "\n",
    "        # Training phase\n",
    "        avg_train_loss = 0\n",
    "        model.train()  # train mode\n",
    "        for X_batch, positive_clicks, negative_clicks in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            # annotated_X = annotated_X.to(device)\n",
    "            print(positive_clicks)\n",
    "            positive_clicks = [[(y, x) for (_, y, x) in pos_click] for pos_click in positive_clicks]\n",
    "            negative_clicks = [[(y, x) for (_, y, x) in neg_click] for neg_click in negative_clicks]\n",
    "            print(positive_clicks)\n",
    "            # set parameter gradients to zero\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            Y_pred = model(X_batch)\n",
    "\n",
    "            # Compute new loss function\n",
    "            total_loss = 0\n",
    "            for i in range(len(Y_pred)):\n",
    "                total_loss += L_point(Y_pred[i], positive_clicks[i], negative_clicks[i])\n",
    "            total_loss /= len(Y_pred) # Average over batch\n",
    "\n",
    "\n",
    "            #loss = loss_fn(Y_batch, Y_pred)  # forward-pass\n",
    "            total_loss.backward()  # backward-pass\n",
    "            opt.step()  # update weights\n",
    "\n",
    "            # calculate metrics to show the user\n",
    "            avg_train_loss += total_loss.item() / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        avg_val_loss = 0\n",
    "        model.eval()  # validation mode\n",
    "        with torch.no_grad():  # disable gradient computation\n",
    "            for X_val, positive_clicks_val, negative_clicks_val in val_loader:\n",
    "                X_val = X_val.to(device)\n",
    "                positive_clicks_val = [[(y, x) for (_, y, x) in pos_click] for pos_click in positive_clicks_val]\n",
    "                negative_clicks_val = [[(y, x) for (_, y, x) in neg_click] for neg_click in negative_clicks_val]\n",
    "\n",
    "                Y_val_pred = model(X_val)\n",
    "\n",
    "                # Compute validation loss\n",
    "                val_total_loss = 0\n",
    "                for i in range(len(Y_val_pred)):\n",
    "                    val_total_loss += L_point(Y_val_pred[i], positive_clicks_val[i], negative_clicks_val[i], alpha=alpha)\n",
    "                avg_val_loss += val_total_loss.item() / len(val_loader)\n",
    "\n",
    "\n",
    "        toc = time()\n",
    "        print(f' - train loss: {avg_train_loss:.4f} - val loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # Adjust learning rate based on validation loss\n",
    "        lr_scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}, saving model.\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save the model checkpoint\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not improve. Patience counter: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered. Training stopped.\")\n",
    "                break\n",
    "\n",
    "        # Show intermediate results\n",
    "         \n",
    "        # Y_hat = torch.sigmoid(model(X_test.to(device))).detach().cpu()\n",
    "        # clear_output(wait=True)\n",
    "        # for k in range(6):\n",
    "        #     plt.subplot(3, 6, k+1)\n",
    "        #     plt.imshow(np.rollaxis(X_test[k].numpy(), 0, 3), cmap='gray')\n",
    "        #     plt.title('Real')\n",
    "        #     plt.axis('off')\n",
    "\n",
    "        #     plt.subplot(3, 6, k+7)\n",
    "        #     plt.imshow(Y_hat[k, 0], cmap='gray')\n",
    "        #     plt.title('Output')\n",
    "        #     plt.axis('off')\n",
    "        #     plt.subplot(3, 6, k+13)\n",
    "        #     plt.imshow(Y_test[k, 0].detach().cpu(), cmap='gray')\n",
    "        #     plt.title('Ground Truth')\n",
    "        #     plt.axis('off')\n",
    "        # plt.suptitle('%d / %d - train loss: %f - val loss: %f' % (epoch+1, epochs, avg_train_loss, avg_val_loss))\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "parameters = 64\n",
    "\n",
    "class UNet2(nn.Module):\n",
    "    def __init__(self, parameter_count = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        def parameters_from_depth(parameters, depth):\n",
    "            return parameters*2*depth\n",
    "        \n",
    "        par0 = parameters_from_depth(parameters, 1)\n",
    "        par1 = parameters_from_depth(parameters, 2)\n",
    "        par2 = parameters_from_depth(parameters, 3)\n",
    "        par3 = parameters_from_depth(parameters, 4)\n",
    "        par4 = parameters_from_depth(parameters, 5)\n",
    "        \n",
    "        \n",
    "        # encoder (downsampling)\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(3, par0, 3, padding=1),\n",
    "            nn.BatchNorm2d(par0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par0, par0, 3,stride=1, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pool0 = nn.Conv2d(par0, par1, 3,stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(par1, par1, 3, padding=1),\n",
    "            nn.BatchNorm2d(par1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par1, par1, 3,stride=1, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.pool1 = nn.Conv2d(par1, par2, 3,stride=2, padding=1)   # 128 -> 64\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(par2, par2, 3, padding=1),\n",
    "            nn.BatchNorm2d(par2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par2, par2, 3,stride=1, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pool2 = nn.Conv2d(par2, par3, 3,stride=2, padding=1)   # 64 -> 32\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(par3, par3, 3, padding=1),\n",
    "            nn.BatchNorm2d(par3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par3, par3, 3,stride=1, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pool3 = nn.Conv2d(par3, par4, 3,stride=2, padding=1)  # 32 -> 16\n",
    "\n",
    "        # bottleneck\n",
    "        self.bottleneck_conv = nn.Sequential(\n",
    "            nn.Conv2d(par4, par4, 3, padding=1),\n",
    "            nn.BatchNorm2d(par4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par4, par4, 3, padding=1),\n",
    "            nn.BatchNorm2d(par4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par4, par4, 3, padding=1),\n",
    "            nn.BatchNorm2d(par4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # decoder (upsampling)\n",
    "        self.upsample0 = nn.ConvTranspose2d(par4, par3, kernel_size=2, stride=2, padding=0) # 16 -> 32\n",
    "        self.dec0 = nn.Sequential(\n",
    "            nn.Conv2d(par3*2, par3, 3, padding=1),\n",
    "            nn.BatchNorm2d(par3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par3, par3, 3, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.upsample1 = nn.ConvTranspose2d(par3, par2, kernel_size=2, stride=2, padding=0)  # 32 -> 64\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(par2*2, par2, 3, padding=1),\n",
    "            nn.BatchNorm2d(par2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par2, par2, 3, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.upsample2 = nn.ConvTranspose2d(par2, par1, kernel_size=2, stride=2, padding=0)  # 64 -> 128\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(par1*2, par1, 3, padding=1),\n",
    "            nn.BatchNorm2d(par1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par1, par1, 3, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.upsample3 = nn.ConvTranspose2d(par1, par0, kernel_size=2, stride=2, padding=0)  # 128 -> 256\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(par0*2, par0, 3, padding=1),\n",
    "            nn.BatchNorm2d(par0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par0, par0, 3, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.final = nn.Conv2d(par0, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x: \", x.shape)\n",
    "        \n",
    "        # encoder\n",
    "        x1 = self.layer0(x)\n",
    "        e0 = self.pool0(x1) # 256 -> 128\n",
    "        # print(\"e0: \", e0.shape)\n",
    "        e1 = self.pool1(self.layer1(e0)) # 128 -> 64\n",
    "        # print(\"e1: \", e1.shape)\n",
    "        \n",
    "        e2 = self.pool2(self.layer2(e1)) # 64 -> 32\n",
    "        # print(\"e2: \", e2.shape)\n",
    "\n",
    "        e3 = self.pool3(self.layer3(e2)) # 32 -> 16\n",
    "        # print(\"e3: \", e3.shape)\n",
    "\n",
    "        # bottleneck\n",
    "        b = self.bottleneck_conv(e3)\n",
    "        # print(\"b: \", b.shape)\n",
    "\n",
    "        # decoder\n",
    "         # decoder\n",
    "        # print(\"d0: \", d0.shape)\n",
    "        d0 = self.upsample0(b)  # Upsample to # 16 -> 32\n",
    "        d0 = torch.cat([d0, e2], dim=1)  # Concatenate with e2 (32x32)\n",
    "        d0 = self.dec0(d0)\n",
    "\n",
    "        d1 = self.upsample1(d0)  # Upsample to # 32 -> 64\n",
    "        d1 = torch.cat([d1, e1], dim=1)  # Concatenate with e1 (64x64)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        d2 = self.upsample2(d1)  # Upsample to # 64 -> 128\n",
    "        d2 = torch.cat([d2, e0], dim=1)  # Concatenate with e0 (128x128)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        \n",
    "        \n",
    "        d3 = self.upsample3(d2)  # Upsample to # 128 -> 256\n",
    "        d3 = torch.cat([d3, x1], dim=1)  # Concatenate with e0 (256x256)\n",
    "        d3 = self.dec3(d3)\n",
    "        d3 = self.final(d3)\n",
    "        return d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import Dice, JaccardIndex, Accuracy, Recall, Specificity\n",
    "\n",
    "#  Dice overlap, Intersection overUnion, Accuracy, Sensitivity, and Specifici\n",
    "\n",
    "def bce_loss(y_real, y_pred):\n",
    "    return torch.mean(y_pred - y_real*y_pred + torch.log(1 + torch.exp(-y_pred)))\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-6  # To avoid division by zero\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "\n",
    "# def dice_coefficient(y_pred, y_true, epsilon=1e-07):\n",
    "#     y_pred_copy = prediction.clone()\n",
    "\n",
    "#     y_pred_copy[prediction_copy < 0] = 0\n",
    "#     y_pred_copy[prediction_copy > 0] = 1\n",
    "\n",
    "#     intersection = abs(torch.sum(y_pred_copy * y_true))\n",
    "#     union = abs(torch.sum(y_pred_copy) + torch.sum(y_true))\n",
    "#     dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "#     return dice\n",
    "\n",
    "# Intersection overUnion\n",
    "\n",
    "def intersection_over_union(y_true, y_pred):\n",
    "    smooth = 1e-6  # To avoid division by zero\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    union = y_true_f.sum() + y_pred_f.sum() - intersection\n",
    "    return intersection / (union + smooth)\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_pred = y_pred.round()  # Convert probabilities to binary\n",
    "    correct = (y_true == y_pred).float()  # Check if predictions are correct\n",
    "    return correct.sum() / correct.numel()\n",
    "\n",
    "# Sensitivity (measures true positives)\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    y_pred = y_pred.round()  # Convert probabilities to binary\n",
    "    true_positives = (y_true * y_pred).sum()\n",
    "    possible_positives = y_true.sum()\n",
    "    return true_positives / (possible_positives + 1e-6)  # Avoid division by zero\n",
    "\n",
    "# Specificity(measures true negatives)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    y_pred = y_pred.round()  # Convert probabilities to binary\n",
    "    true_negatives = ((1 - y_true) * (1 - y_pred)).sum()\n",
    "    possible_negatives = (1 - y_true).sum()\n",
    "    return true_negatives / (possible_negatives + 1e-6)  # Avoid division by zero\n",
    "\n",
    "\n",
    "def evaluate_model_with_metric(model, device, test_loader, metric_fn):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_metric = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X_batch, Y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)  # Move data to the same device as the model (e.g., GPU)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "\n",
    "            # Forward pass: Get model predictions\n",
    "            Y_pred = model(X_batch)\n",
    "\n",
    "            # Calculate the metric for this batch using the provided metric function\n",
    "            metric_value = metric_fn(Y_batch, Y_pred)\n",
    "            total_metric += metric_value.item() * X_batch.size(0)  # Sum the metric over the batch\n",
    "\n",
    "            total_samples += X_batch.size(0)\n",
    "\n",
    "    # Compute average metric value over the entire test set\n",
    "    avg_metric = total_metric / total_samples\n",
    "    #print(f'Final Model Performance - Average Metric: {avg_metric:.4f}')\n",
    "    \n",
    "    return avg_metric\n",
    "\n",
    "\n",
    "\n",
    "def calculate_segmentation_metrics(y_true,y_pred, device):\n",
    "    y_pred = (y_pred > 0.5).long()  # Convert probabilities to binary\n",
    "    y_true = y_true.long() \n",
    "\n",
    "    #dice_score2=Dice(y_pred, y_true)\n",
    "    dice_func = Dice().to(device)\n",
    "    dice_score  = dice_func(y_pred,y_true)\n",
    "    iou_func = JaccardIndex(task=\"binary\").to(device)\n",
    "    iou_score = iou_func(y_pred,y_true)\n",
    "    accuracy_func = Accuracy(task=\"binary\").to(device)\n",
    "    accuracy_score = accuracy_func(y_pred,y_true)\n",
    "    recall_func = Recall(task=\"binary\").to(device)\n",
    "    sensitivity_score=recall_func(y_pred,y_true)\n",
    "    specificity_func = Specificity(task=\"binary\").to(device)\n",
    "    specificity_score=specificity_func(y_pred,y_true)\n",
    "    print (dice_score)\n",
    "    print(iou_score)\n",
    "    print(accuracy_score)\n",
    "    print(sensitivity_score)\n",
    "    print(specificity_score)\n",
    "    metrics = {\n",
    "            'Dice': dice_score,\n",
    "            'IoU': iou_score,\n",
    "            'Accuracy': accuracy_score,\n",
    "            'Sensitivity': sensitivity_score,\n",
    "            'Specificity': specificity_score\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists to store metrics for each batch\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    accuracy_scores = []\n",
    "    sensitivity_scores = []\n",
    "    specificity_scores = []\n",
    "    \n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Assuming each batch has images and masks\n",
    "            images, masks = batch\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # Forward pass to get predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert outputs to probabilities if necessary\n",
    "            y_pred = torch.sigmoid(outputs)  # Assuming binary segmentation\n",
    "            \n",
    "            # Calculate metrics for this batch\n",
    "            metrics = calculate_segmentation_metrics(masks, y_pred, device)\n",
    "            \n",
    "            # Append each metric\n",
    "            dice_scores.append(metrics['Dice'].item())\n",
    "            iou_scores.append(metrics['IoU'].item())\n",
    "            accuracy_scores.append(metrics['Accuracy'].item())\n",
    "            sensitivity_scores.append(metrics['Sensitivity'].item())\n",
    "            specificity_scores.append(metrics['Specificity'].item())\n",
    "    \n",
    "    # Compute average for each metric\n",
    "    avg_metrics = {\n",
    "        'Dice': sum(dice_scores) / len(dice_scores),\n",
    "        'IoU': sum(iou_scores) / len(iou_scores),\n",
    "        'Accuracy': sum(accuracy_scores) / len(accuracy_scores),\n",
    "        'Sensitivity': sum(sensitivity_scores) / len(sensitivity_scores),\n",
    "        'Specificity': sum(specificity_scores) / len(specificity_scores)\n",
    "    }\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "#loss function for abalation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training samples: 140\n",
      "Validation samples: 30\n",
      "Testing samples: 30\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 256, 256]           3,584\n",
      "       BatchNorm2d-2        [-1, 128, 256, 256]             256\n",
      "              ReLU-3        [-1, 128, 256, 256]               0\n",
      "            Conv2d-4        [-1, 128, 256, 256]         147,584\n",
      "       BatchNorm2d-5        [-1, 128, 256, 256]             256\n",
      "              ReLU-6        [-1, 128, 256, 256]               0\n",
      "            Conv2d-7        [-1, 256, 128, 128]         295,168\n",
      "            Conv2d-8        [-1, 256, 128, 128]         590,080\n",
      "       BatchNorm2d-9        [-1, 256, 128, 128]             512\n",
      "             ReLU-10        [-1, 256, 128, 128]               0\n",
      "           Conv2d-11        [-1, 256, 128, 128]         590,080\n",
      "      BatchNorm2d-12        [-1, 256, 128, 128]             512\n",
      "             ReLU-13        [-1, 256, 128, 128]               0\n",
      "           Conv2d-14          [-1, 384, 64, 64]         885,120\n",
      "           Conv2d-15          [-1, 384, 64, 64]       1,327,488\n",
      "      BatchNorm2d-16          [-1, 384, 64, 64]             768\n",
      "             ReLU-17          [-1, 384, 64, 64]               0\n",
      "           Conv2d-18          [-1, 384, 64, 64]       1,327,488\n",
      "      BatchNorm2d-19          [-1, 384, 64, 64]             768\n",
      "             ReLU-20          [-1, 384, 64, 64]               0\n",
      "           Conv2d-21          [-1, 512, 32, 32]       1,769,984\n",
      "           Conv2d-22          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-23          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-24          [-1, 512, 32, 32]               0\n",
      "           Conv2d-25          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-26          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-27          [-1, 512, 32, 32]               0\n",
      "           Conv2d-28          [-1, 640, 16, 16]       2,949,760\n",
      "           Conv2d-29          [-1, 640, 16, 16]       3,687,040\n",
      "      BatchNorm2d-30          [-1, 640, 16, 16]           1,280\n",
      "             ReLU-31          [-1, 640, 16, 16]               0\n",
      "           Conv2d-32          [-1, 640, 16, 16]       3,687,040\n",
      "      BatchNorm2d-33          [-1, 640, 16, 16]           1,280\n",
      "             ReLU-34          [-1, 640, 16, 16]               0\n",
      "           Conv2d-35          [-1, 640, 16, 16]       3,687,040\n",
      "      BatchNorm2d-36          [-1, 640, 16, 16]           1,280\n",
      "             ReLU-37          [-1, 640, 16, 16]               0\n",
      "  ConvTranspose2d-38          [-1, 512, 32, 32]       1,311,232\n",
      "           Conv2d-39          [-1, 512, 32, 32]       4,719,104\n",
      "      BatchNorm2d-40          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-41          [-1, 512, 32, 32]               0\n",
      "           Conv2d-42          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-43          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-44          [-1, 512, 32, 32]               0\n",
      "  ConvTranspose2d-45          [-1, 384, 64, 64]         786,816\n",
      "           Conv2d-46          [-1, 384, 64, 64]       2,654,592\n",
      "      BatchNorm2d-47          [-1, 384, 64, 64]             768\n",
      "             ReLU-48          [-1, 384, 64, 64]               0\n",
      "           Conv2d-49          [-1, 384, 64, 64]       1,327,488\n",
      "      BatchNorm2d-50          [-1, 384, 64, 64]             768\n",
      "             ReLU-51          [-1, 384, 64, 64]               0\n",
      "  ConvTranspose2d-52        [-1, 256, 128, 128]         393,472\n",
      "           Conv2d-53        [-1, 256, 128, 128]       1,179,904\n",
      "      BatchNorm2d-54        [-1, 256, 128, 128]             512\n",
      "             ReLU-55        [-1, 256, 128, 128]               0\n",
      "           Conv2d-56        [-1, 256, 128, 128]         590,080\n",
      "      BatchNorm2d-57        [-1, 256, 128, 128]             512\n",
      "             ReLU-58        [-1, 256, 128, 128]               0\n",
      "  ConvTranspose2d-59        [-1, 128, 256, 256]         131,200\n",
      "           Conv2d-60        [-1, 128, 256, 256]         295,040\n",
      "      BatchNorm2d-61        [-1, 128, 256, 256]             256\n",
      "             ReLU-62        [-1, 128, 256, 256]               0\n",
      "           Conv2d-63        [-1, 128, 256, 256]         147,584\n",
      "      BatchNorm2d-64        [-1, 128, 256, 256]             256\n",
      "             ReLU-65        [-1, 128, 256, 256]               0\n",
      "           Conv2d-66          [-1, 1, 256, 256]           1,153\n",
      "================================================================\n",
      "Total params: 41,578,625\n",
      "Trainable params: 41,578,625\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 1517.00\n",
      "Params size (MB): 158.61\n",
      "Estimated Total Size (MB): 1676.36\n",
      "----------------------------------------------------------------\n",
      "* Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samer\\Documents\\Intro to deep learning for computer vision\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  0,  73,  47],\n",
      "        [  0,  73,  89],\n",
      "        [  0,  61,  88],\n",
      "        [  0,  65,  41],\n",
      "        [  0,  52,  99],\n",
      "        [  0,  85,  73],\n",
      "        [  0, 104,  62],\n",
      "        [  0, 109,  70],\n",
      "        [  0,  97,  21],\n",
      "        [  0,  69,  82],\n",
      "        [  0,  74,  55],\n",
      "        [  0,  59,  61],\n",
      "        [  0,  74,  66],\n",
      "        [  0,  93,  80],\n",
      "        [  0,  17,  83],\n",
      "        [  0,  82,  73],\n",
      "        [  0,  75,  59],\n",
      "        [  0,  49,  89],\n",
      "        [  0,  50,  74],\n",
      "        [  0,  94,  16],\n",
      "        [  0,  46,  70],\n",
      "        [  0,  82,  95],\n",
      "        [  0,  19,  66],\n",
      "        [  0,  82,  48],\n",
      "        [  0,  53,  65],\n",
      "        [  0,  90,  86],\n",
      "        [  0,  70,  56],\n",
      "        [  0,  50,  35],\n",
      "        [  0,  70,  70],\n",
      "        [  0,  30,  74],\n",
      "        [  0,  79,  59],\n",
      "        [  0,  71,  51]]), tensor([[  0,  66,  43],\n",
      "        [  0,  48,  75],\n",
      "        [  0,  29,  63],\n",
      "        [  0,  35,  60],\n",
      "        [  0,  74,  80],\n",
      "        [  0,   6,  83],\n",
      "        [  0,  83,  56],\n",
      "        [  0,  69,  44],\n",
      "        [  0,  92,  52],\n",
      "        [  0,  64,  80],\n",
      "        [  0,  58,  40],\n",
      "        [  0,  58,  64],\n",
      "        [  0,  84,  96],\n",
      "        [  0,  62,   7],\n",
      "        [  0,  53,  35],\n",
      "        [  0,  67,  98],\n",
      "        [  0,  83,  71],\n",
      "        [  0,  95,  82],\n",
      "        [  0,  55,  74],\n",
      "        [  0, 126,  63],\n",
      "        [  0,  72,  58],\n",
      "        [  0,  70,  77],\n",
      "        [  0,  54,  87],\n",
      "        [  0,  62,  83],\n",
      "        [  0,  28,  14],\n",
      "        [  0,  65,  66],\n",
      "        [  0,  85,  57],\n",
      "        [  0,  99,  34],\n",
      "        [  0,  63,  54],\n",
      "        [  0,  88,  44],\n",
      "        [  0,  19,  78],\n",
      "        [  0,  17,  82]]), tensor([[  0,  47,  49],\n",
      "        [  0,  93,  93],\n",
      "        [  0,  75,  53],\n",
      "        [  0,  67,  53],\n",
      "        [  0,  64,  84],\n",
      "        [  0, 116,  78],\n",
      "        [  0,  45,  56],\n",
      "        [  0,  91,  82],\n",
      "        [  0,  34,  42],\n",
      "        [  0,  80,  74],\n",
      "        [  0,  62,  59],\n",
      "        [  0,  73,  67],\n",
      "        [  0,  22,  39],\n",
      "        [  0,  74,  18],\n",
      "        [  0,  69,  27],\n",
      "        [  0,  81,  76],\n",
      "        [  0,  66,  89],\n",
      "        [  0,  22,  11],\n",
      "        [  0,  59,  77],\n",
      "        [  0,  98,  47],\n",
      "        [  0,  48,  47],\n",
      "        [  0,  75,  81],\n",
      "        [  0, 101,  51],\n",
      "        [  0,  49,  84],\n",
      "        [  0, 102,  37],\n",
      "        [  0,  91,  70],\n",
      "        [  0,  44,  71],\n",
      "        [  0,  51,  69],\n",
      "        [  0, 110,  33],\n",
      "        [  0, 104,  36],\n",
      "        [  0,  55,  53],\n",
      "        [  0,  52,  94]]), tensor([[  0,  65,  42],\n",
      "        [  0,  85,  89],\n",
      "        [  0,  47,  73],\n",
      "        [  0,  43,  59],\n",
      "        [  0,  26,  97],\n",
      "        [  0,  49, 102],\n",
      "        [  0,  46,  69],\n",
      "        [  0,  60,  55],\n",
      "        [  0,  81,  11],\n",
      "        [  0,  71,  76],\n",
      "        [  0,  44,  75],\n",
      "        [  0,  93,  48],\n",
      "        [  0,  81,  45],\n",
      "        [  0,  82,  81],\n",
      "        [  0,  54,  52],\n",
      "        [  0,  89,  59],\n",
      "        [  0,  54,  76],\n",
      "        [  0,  37,  39],\n",
      "        [  0,  57,  45],\n",
      "        [  0,  37,  61],\n",
      "        [  0,  81,  57],\n",
      "        [  0,  43,  97],\n",
      "        [  0,  53,  52],\n",
      "        [  0,  57,  89],\n",
      "        [  0,  70,  91],\n",
      "        [  0,  62,  56],\n",
      "        [  0,  71,  52],\n",
      "        [  0,  80,  88],\n",
      "        [  0,  20,  84],\n",
      "        [  0,  23,  21],\n",
      "        [  0,  47,  88],\n",
      "        [  0,  24,  92]]), tensor([[  0,  55,  29],\n",
      "        [  0,  65,  58],\n",
      "        [  0,  26,  87],\n",
      "        [  0,  48,  47],\n",
      "        [  0,  63,  83],\n",
      "        [  0,  94,  47],\n",
      "        [  0,  90,  74],\n",
      "        [  0,  61,  78],\n",
      "        [  0,  39,   1],\n",
      "        [  0,  62,  68],\n",
      "        [  0,  33,  64],\n",
      "        [  0,  43,  91],\n",
      "        [  0,  65,  38],\n",
      "        [  0,  17,  40],\n",
      "        [  0,  23,  42],\n",
      "        [  0,  39,  60],\n",
      "        [  0,  58,  55],\n",
      "        [  0,  22,  21],\n",
      "        [  0,  36,  61],\n",
      "        [  0, 107,  82],\n",
      "        [  0,  36,  47],\n",
      "        [  0,   9, 101],\n",
      "        [  0,  74,  25],\n",
      "        [  0,  97,  56],\n",
      "        [  0,  29, 122],\n",
      "        [  0,  79,  70],\n",
      "        [  0,  76,  87],\n",
      "        [  0,  58,  25],\n",
      "        [  0,  13,  58],\n",
      "        [  0,  36,  12],\n",
      "        [  0,  48,  34],\n",
      "        [  0,  72,  48]])]\n",
      "[[(tensor(73), tensor(47)), (tensor(73), tensor(89)), (tensor(61), tensor(88)), (tensor(65), tensor(41)), (tensor(52), tensor(99)), (tensor(85), tensor(73)), (tensor(104), tensor(62)), (tensor(109), tensor(70)), (tensor(97), tensor(21)), (tensor(69), tensor(82)), (tensor(74), tensor(55)), (tensor(59), tensor(61)), (tensor(74), tensor(66)), (tensor(93), tensor(80)), (tensor(17), tensor(83)), (tensor(82), tensor(73)), (tensor(75), tensor(59)), (tensor(49), tensor(89)), (tensor(50), tensor(74)), (tensor(94), tensor(16)), (tensor(46), tensor(70)), (tensor(82), tensor(95)), (tensor(19), tensor(66)), (tensor(82), tensor(48)), (tensor(53), tensor(65)), (tensor(90), tensor(86)), (tensor(70), tensor(56)), (tensor(50), tensor(35)), (tensor(70), tensor(70)), (tensor(30), tensor(74)), (tensor(79), tensor(59)), (tensor(71), tensor(51))], [(tensor(66), tensor(43)), (tensor(48), tensor(75)), (tensor(29), tensor(63)), (tensor(35), tensor(60)), (tensor(74), tensor(80)), (tensor(6), tensor(83)), (tensor(83), tensor(56)), (tensor(69), tensor(44)), (tensor(92), tensor(52)), (tensor(64), tensor(80)), (tensor(58), tensor(40)), (tensor(58), tensor(64)), (tensor(84), tensor(96)), (tensor(62), tensor(7)), (tensor(53), tensor(35)), (tensor(67), tensor(98)), (tensor(83), tensor(71)), (tensor(95), tensor(82)), (tensor(55), tensor(74)), (tensor(126), tensor(63)), (tensor(72), tensor(58)), (tensor(70), tensor(77)), (tensor(54), tensor(87)), (tensor(62), tensor(83)), (tensor(28), tensor(14)), (tensor(65), tensor(66)), (tensor(85), tensor(57)), (tensor(99), tensor(34)), (tensor(63), tensor(54)), (tensor(88), tensor(44)), (tensor(19), tensor(78)), (tensor(17), tensor(82))], [(tensor(47), tensor(49)), (tensor(93), tensor(93)), (tensor(75), tensor(53)), (tensor(67), tensor(53)), (tensor(64), tensor(84)), (tensor(116), tensor(78)), (tensor(45), tensor(56)), (tensor(91), tensor(82)), (tensor(34), tensor(42)), (tensor(80), tensor(74)), (tensor(62), tensor(59)), (tensor(73), tensor(67)), (tensor(22), tensor(39)), (tensor(74), tensor(18)), (tensor(69), tensor(27)), (tensor(81), tensor(76)), (tensor(66), tensor(89)), (tensor(22), tensor(11)), (tensor(59), tensor(77)), (tensor(98), tensor(47)), (tensor(48), tensor(47)), (tensor(75), tensor(81)), (tensor(101), tensor(51)), (tensor(49), tensor(84)), (tensor(102), tensor(37)), (tensor(91), tensor(70)), (tensor(44), tensor(71)), (tensor(51), tensor(69)), (tensor(110), tensor(33)), (tensor(104), tensor(36)), (tensor(55), tensor(53)), (tensor(52), tensor(94))], [(tensor(65), tensor(42)), (tensor(85), tensor(89)), (tensor(47), tensor(73)), (tensor(43), tensor(59)), (tensor(26), tensor(97)), (tensor(49), tensor(102)), (tensor(46), tensor(69)), (tensor(60), tensor(55)), (tensor(81), tensor(11)), (tensor(71), tensor(76)), (tensor(44), tensor(75)), (tensor(93), tensor(48)), (tensor(81), tensor(45)), (tensor(82), tensor(81)), (tensor(54), tensor(52)), (tensor(89), tensor(59)), (tensor(54), tensor(76)), (tensor(37), tensor(39)), (tensor(57), tensor(45)), (tensor(37), tensor(61)), (tensor(81), tensor(57)), (tensor(43), tensor(97)), (tensor(53), tensor(52)), (tensor(57), tensor(89)), (tensor(70), tensor(91)), (tensor(62), tensor(56)), (tensor(71), tensor(52)), (tensor(80), tensor(88)), (tensor(20), tensor(84)), (tensor(23), tensor(21)), (tensor(47), tensor(88)), (tensor(24), tensor(92))], [(tensor(55), tensor(29)), (tensor(65), tensor(58)), (tensor(26), tensor(87)), (tensor(48), tensor(47)), (tensor(63), tensor(83)), (tensor(94), tensor(47)), (tensor(90), tensor(74)), (tensor(61), tensor(78)), (tensor(39), tensor(1)), (tensor(62), tensor(68)), (tensor(33), tensor(64)), (tensor(43), tensor(91)), (tensor(65), tensor(38)), (tensor(17), tensor(40)), (tensor(23), tensor(42)), (tensor(39), tensor(60)), (tensor(58), tensor(55)), (tensor(22), tensor(21)), (tensor(36), tensor(61)), (tensor(107), tensor(82)), (tensor(36), tensor(47)), (tensor(9), tensor(101)), (tensor(74), tensor(25)), (tensor(97), tensor(56)), (tensor(29), tensor(122)), (tensor(79), tensor(70)), (tensor(76), tensor(87)), (tensor(58), tensor(25)), (tensor(13), tensor(58)), (tensor(36), tensor(12)), (tensor(48), tensor(34)), (tensor(72), tensor(48))]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet2()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     45\u001b[0m summary(model, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtrain_with_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#do evaluate model performace on loss functios\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#avg_dice = evaluate_model_with_metric(model, device, test_loader, dice_coefficient)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# specificity_ = evaluate_model_with_metric(model, device, test_loader, specificity)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# print(f'Final Model Performance - Specificity Metric: {specificity_:.4f}')\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Model Performance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mtrain_with_validation\u001b[1;34m(device, model, opt, loss_fn, epochs, train_loader, val_loader, test_loader, patience)\u001b[0m\n\u001b[0;32m     34\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Y_pred)):\n\u001b[1;32m---> 36\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m L_point(Y_pred[i], \u001b[43mpositive_clicks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, negative_clicks[i])\n\u001b[0;32m     37\u001b[0m total_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y_pred) \u001b[38;5;66;03m# Average over batch\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#loss = loss_fn(Y_batch, Y_pred)  # forward-pass\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#!/zhome/44/9/212447/venv_1/bin/python3\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "# pip install torchsummary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "data_path = \"./dtu/datasets1/02516/PH2_Dataset_images\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),    # Resize to a fixed size\n",
    "    transforms.ToTensor()             # Convert to tensor\n",
    "])\n",
    "# Create dataset instances for train, validation, and test\n",
    "train_dataset = SkinLesionLoader(transform=transform, dataset_path=data_path, split='train')\n",
    "val_dataset = SkinLesionLoader(transform=transform, dataset_path=data_path, split='val')\n",
    "test_dataset = SkinLesionLoader(transform=transform, dataset_path=data_path, split='test')\n",
    "\n",
    "#Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "model = UNet2().to(device)\n",
    "summary(model, (3, 256, 256))\n",
    "\n",
    "train_with_validation(device, model, optim.Adam(model.parameters()), L_point, 20, train_loader, val_loader, test_loader)\n",
    "\n",
    "#do evaluate model performace on loss functios\n",
    "\n",
    "#avg_dice = evaluate_model_with_metric(model, device, test_loader, dice_coefficient)\n",
    "# print(f'Final Model Performance - Dice Coefficient Metric: {avg_dice:.4f}')\n",
    "# intersect = evaluate_model_with_metric(model, device, test_loader, intersection_over_union)\n",
    "# print(f'Final Model Performance - Intersection Over Union Metric: {intersect:.4f}')\n",
    "# accuracy_ = evaluate_model_with_metric(model, device, test_loader, accuracy)\n",
    "# print(f'Final Model Performance - Accuracy Metric: {accuracy_:.4f}')\n",
    "# sensitivity_ = evaluate_model_with_metric(model, device, test_loader, sensitivity)\n",
    "# print(f'Final Model Performance - Sensitivity Metric: {sensitivity_:.4f}')\n",
    "# specificity_ = evaluate_model_with_metric(model, device, test_loader, specificity)\n",
    "# print(f'Final Model Performance - Specificity Metric: {specificity_:.4f}')\n",
    "\n",
    "print(\"Final Model Performance\")\n",
    "metrics = evaluate_model(model, test_loader, device)\n",
    "for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value:.4f}\")  # Format to 4 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
