{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "def generate_clicks(mask, num_positives, num_negatives, margin, var):\n",
    "\n",
    "    # Get object (foregorund) and background pixels\n",
    "    object_pixels = np.argwhere(mask==True)\n",
    "    lesion_dilated = binary_dilation(mask, iterations=margin)  # Dilate lesion area by margin\n",
    "    background_pixels = np.argwhere((mask == False) & (~lesion_dilated))\n",
    "    \n",
    "    # Function to check if the new point is sufficiently distant from existing points\n",
    "    def is_dispersed(new_point, existing_points, min_distance):\n",
    "        for point in existing_points:\n",
    "            if np.linalg.norm(new_point - point) < min_distance:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Sample positive clicks from object pixels\n",
    "    positive_clicks=[] # List of random positions for positive clicks\n",
    "    while len(positive_clicks) < num_positives:\n",
    "        new_pos_click = random.choice(object_pixels)\n",
    "        if is_dispersed(new_pos_click, positive_clicks, var):\n",
    "            positive_clicks.append(new_pos_click)\n",
    "\n",
    "    # Sample negative clicks from background pixels\n",
    "    negative_clicks=[]\n",
    "    while len(negative_clicks) < num_negatives:\n",
    "        new_neg_click= random.choice(background_pixels)\n",
    "        if is_dispersed(new_neg_click, negative_clicks, var):\n",
    "            negative_clicks.append(new_neg_click)\n",
    "\n",
    "    return positive_clicks, negative_clicks\n",
    "\n",
    "# Define the function to draw the ramdom points in the mask\n",
    "\n",
    "def draw_clicks_on_original(image_path, positive_clicks, negative_clicks, circle_radius,border_thickness):\n",
    "    # Create a copy of the mask to draw on\n",
    "    original_image = Image.open(image_path).convert('RGB')  \n",
    "    draw = ImageDraw.Draw(original_image)\n",
    "\n",
    "    # Draw positive clicks (in green)\n",
    "    for (y,x) in positive_clicks:\n",
    "        # Draw outer circle for black border\n",
    "        draw.ellipse((x - circle_radius - border_thickness, y - circle_radius - border_thickness,\n",
    "                      x + circle_radius + border_thickness, y + circle_radius + border_thickness), fill=\"black\")\n",
    "        # Draw inner green circle\n",
    "        draw.ellipse((x- circle_radius, y-circle_radius, x + circle_radius, y+ circle_radius), outline='green', fill='green') \n",
    "\n",
    "    for (y,x) in negative_clicks:\n",
    "        # Draw outer circle for black border\n",
    "        draw.ellipse((x - circle_radius - border_thickness, y - circle_radius - border_thickness,\n",
    "                      x + circle_radius + border_thickness, y + circle_radius + border_thickness), fill=\"black\")\n",
    "        # Draw inner red circle\n",
    "        draw.ellipse((x-circle_radius, y-circle_radius, x + circle_radius, y + circle_radius), outline='red', fill='red')\n",
    "\n",
    "    return original_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def generate_clustered_annotations(mask, num_clusters_pos, num_clusters_neg, margin):\n",
    "    # Obtener índices de la lesión y el fondo\n",
    "    lesion_pixels = np.argwhere(mask == True)\n",
    "    background_pixels = np.argwhere(mask == False)\n",
    "    \n",
    "    # Aplicar K-Means para los píxeles de la lesión\n",
    "    kmeans = KMeans(n_clusters=num_clusters_pos, random_state=0).fit(lesion_pixels)\n",
    "    positive_clicks = kmeans.cluster_centers_.astype(int)  # Centroides de los clusters\n",
    "    \n",
    "    # Filtrar los índices de fondo con un margen\n",
    "    from scipy.ndimage import binary_dilation\n",
    "    lesion_dilated = binary_dilation(mask, iterations=margin)\n",
    "    background_pixels_margin = np.argwhere((mask == 0) & (~lesion_dilated))\n",
    "    \n",
    "    # Seleccionar puntos negativos aleatorios\n",
    "    kmeans_background = KMeans(n_clusters=num_clusters_neg, random_state=0).fit(background_pixels_margin)\n",
    "    negative_clicks = kmeans_background.cluster_centers_.astype(int) \n",
    "\n",
    "    return positive_clicks, negative_clicks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the Data Loader using ramdom points( We leave it the same way)\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SkinLesionLoader(Dataset):\n",
    "    def __init__(self, transform, dataset_path, num_positives=5, num_negatives=5, margin=10, var=20, circle_radius=5, border_thickness=2, split='train'):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        self.num_positives = num_positives\n",
    "        self.num_negatives = num_negatives\n",
    "        self.margin = margin\n",
    "        self.var = var\n",
    "        self.circle_radius = circle_radius\n",
    "        self.border_thickness = border_thickness\n",
    "        \n",
    "        # Collect all image paths and label paths\n",
    "        self.image_paths = []\n",
    "        self.label_paths = []\n",
    "\n",
    "        # Loop through all IMG### folders\n",
    "        for folder_name in os.listdir(dataset_path):\n",
    "            folder_path = os.path.join(dataset_path, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                # Add dermoscopic image path\n",
    "                dermoscopic_image_path = os.path.join(folder_path, f'{folder_name}_Dermoscopic_Image', f'{folder_name}.bmp')\n",
    "                if os.path.exists(dermoscopic_image_path):\n",
    "                    self.image_paths.append(dermoscopic_image_path)\n",
    "                \n",
    "                # Add lesion image path\n",
    "                lesion_image_path = os.path.join(folder_path, f'{folder_name}_lesion', f'{folder_name}_lesion.bmp')\n",
    "                if os.path.exists(lesion_image_path):\n",
    "                    self.label_paths.append(lesion_image_path)\n",
    "\n",
    "        # Ensure both lists have the same length\n",
    "        assert len(self.image_paths) == len(self.label_paths), \"Image and label counts do not match.\"\n",
    "\n",
    "        # Randomly shuffle the dataset\n",
    "        combined = list(zip(self.image_paths, self.label_paths))\n",
    "        random.shuffle(combined)\n",
    "        self.image_paths, self.label_paths = zip(*combined)\n",
    "\n",
    "        # Split indices for 70-15-15\n",
    "        total_len = len(self.image_paths)\n",
    "        train_split = int(total_len * 0.7)\n",
    "        val_split = int(total_len * 0.85)  # 70% for training, next 15% for validation\n",
    "\n",
    "        if split == 'train':\n",
    "            self.image_paths = self.image_paths[:train_split]\n",
    "            self.label_paths = self.label_paths[:train_split]\n",
    "        elif split == 'val':\n",
    "            self.image_paths = self.image_paths[train_split:val_split]\n",
    "            self.label_paths = self.label_paths[train_split:val_split]\n",
    "        elif split == 'test':\n",
    "            self.image_paths = self.image_paths[val_split:]\n",
    "            self.label_paths = self.label_paths[val_split:]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split name. Use 'train', 'val', or 'test'.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Ensure image is in RGB format\n",
    "        mask = Image.open(label_path).convert(\"L\")    # Ensure label is in grayscale\n",
    "\n",
    "        X = self.transform(image)\n",
    "        Y = self.transform(mask)\n",
    "\n",
    "        # Convert mask to binary format for lesion/background distinction\n",
    "        mask_array = np.array(Y) > 0\n",
    "\n",
    "        # Generate positive and negative clicks using the mask\n",
    "        positive_clicks, negative_clicks = generate_clicks(mask_array, self.num_positives, self.num_negatives, self.margin, self.var)\n",
    "\n",
    "        # Draw clicks on the original image\n",
    "        annotated_image = draw_clicks_on_original(image_path, positive_clicks, negative_clicks, self.circle_radius, self.border_thickness)\n",
    "        \n",
    "        \n",
    "        return X, annotated_image, positive_clicks, negative_clicks\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Clics Negativos:\u001b[39m\u001b[38;5;124m\"\u001b[39m, negative_clicks_batch[i])\n\u001b[1;32m     33\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtest_skin_lesion_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/dtu/datasets1/02516//PH2_Dataset_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [25], line 13\u001b[0m, in \u001b[0;36mtest_skin_lesion_loader\u001b[0;34m(data_path, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Obtiene un lote\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m X_batch, annotated_images, positive_clicks_batch, negative_clicks_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Visualiza cada imagen en el lote junto con las anotaciones de los clics\u001b[39;00m\n\u001b[1;32m     16\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, batch_size, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn [24], line 86\u001b[0m, in \u001b[0;36mSkinLesionLoader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     83\u001b[0m positive_clicks, negative_clicks \u001b[38;5;241m=\u001b[39m generate_clicks(mask_array, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_positives, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_negatives, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Draw clicks on the original image\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_clicks_on_original\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_clicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_clicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircle_radius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mborder_thickness\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "Cell \u001b[0;32mIn [1], line 45\u001b[0m, in \u001b[0;36mdraw_clicks_on_original\u001b[0;34m(image_path, positive_clicks, negative_clicks, circle_radius, border_thickness)\u001b[0m\n\u001b[1;32m     42\u001b[0m draw \u001b[38;5;241m=\u001b[39m ImageDraw\u001b[38;5;241m.\u001b[39mDraw(original_image)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Draw positive clicks (in green)\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (y,x) \u001b[38;5;129;01min\u001b[39;00m positive_clicks:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Draw outer circle for black border\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     draw\u001b[38;5;241m.\u001b[39mellipse((x \u001b[38;5;241m-\u001b[39m circle_radius \u001b[38;5;241m-\u001b[39m border_thickness, y \u001b[38;5;241m-\u001b[39m circle_radius \u001b[38;5;241m-\u001b[39m border_thickness,\n\u001b[1;32m     48\u001b[0m                   x \u001b[38;5;241m+\u001b[39m circle_radius \u001b[38;5;241m+\u001b[39m border_thickness, y \u001b[38;5;241m+\u001b[39m circle_radius \u001b[38;5;241m+\u001b[39m border_thickness), fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Draw inner green circle\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def test_skin_lesion_loader(data_path, batch_size=1):\n",
    "    # Transforms para preprocesamiento\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Redimensiona las imágenes\n",
    "        transforms.ToTensor()           # Convierte a tensor\n",
    "    ])\n",
    "    \n",
    "    # Carga el conjunto de datos con los parámetros necesarios\n",
    "    dataset = SkinLesionLoader(transform=transform, dataset_path=data_path, split='train')\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Obtiene un lote\n",
    "    X_batch, annotated_images, positive_clicks_batch, negative_clicks_batch = next(iter(data_loader))\n",
    "\n",
    "    # Visualiza cada imagen en el lote junto con las anotaciones de los clics\n",
    "    fig, axes = plt.subplots(2, batch_size, figsize=(15, 6))\n",
    "    for i in range(batch_size):\n",
    "        # Muestra la imagen original transformada\n",
    "        axes[0, i].imshow(X_batch[i].permute(1, 2, 0))\n",
    "        axes[0, i].set_title(\"Imagen Procesada\")\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        # Muestra la imagen anotada con clics\n",
    "        axes[1, i].imshow(annotated_images[i].permute(1, 2, 0))\n",
    "        axes[1, i].set_title(\"Imagen con Clics\")\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "        # Muestra las coordenadas de clics generadas\n",
    "        print(f\"Imagen {i+1}:\")\n",
    "        print(\" Clics Positivos:\", positive_clicks_batch[i])\n",
    "        print(\" Clics Negativos:\", negative_clicks_batch[i])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "test_skin_lesion_loader(\"/dtu/datasets1/02516//PH2_Dataset_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def L_point(output, pos_clicks_position, neg_clicks_position, alpha=0.75):\n",
    "    # We extract the known points for the mask, but for the ouput\n",
    "    positive_preds=torch.stack([output[y, x] for (x, y) in pos_clicks_position])\n",
    "    negative_preds = torch.stack([output[y, x] for (x, y) in neg_clicks_position])\n",
    "\n",
    "    # We use the BCE Loss\n",
    "    bce_loss= nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Labels: 1 for positive and 0 for negative\n",
    "    positive_labels = torch.ones_like(positive_preds)\n",
    "    negative_labels = torch.zeros_like(negative_preds)\n",
    "\n",
    "    # Calculate the BCE for the positive and negatives\n",
    "    loss_positive = bce_loss(positive_preds, positive_labels)\n",
    "    loss_negative = bce_loss(negative_preds, negative_labels)\n",
    "\n",
    "\n",
    "    # Calculate the curretn loss adding all the losses for the positives and negatives\n",
    "    total_loss = alpha*loss_positive + (1-alpha)*loss_negative\n",
    "    return total_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(output, target, alpha=0.25, gamma=2.0):\n",
    "   \n",
    "    # Convertir logits a probabilidades\n",
    "    probs = torch.sigmoid(output)\n",
    "    \n",
    "    # Calcular la pérdida focal\n",
    "    loss = -alpha * (1 - probs) ** gamma * (target * torch.log(probs + 1e-8) + (1 - target) * torch.log(1 - probs + 1e-8))\n",
    "    \n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the training function accordingly\n",
    "\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_with_validation(device, model, opt, loss_fn, epochs, train_loader, val_loader, test_loader, patience=5):\n",
    "    #X_test, Y_test, num_pos_test, num_neg_test = next(iter(test_loader))\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tic = time()\n",
    "        print('* Epoch %d/%d' % (epoch+1, epochs))\n",
    "\n",
    "        # Training phase\n",
    "        avg_train_loss = 0\n",
    "        model.train()  # train mode\n",
    "        for X_batch, annotated_X, positive_clicks, negative_clicks in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            annotated_X = annotated_X.to(device)\n",
    "            positive_clicks = [[(y,x) for (y,x) in pos_click] for pos_click in positive_clicks]\n",
    "            negative_clicks = [[(y, x) for (y, x) in neg_click] for neg_click in negative_clicks]\n",
    "\n",
    "            # set parameter gradients to zero\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            Y_pred = model(X_batch)\n",
    "\n",
    "            # Compute new loss function\n",
    "            total_loss = 0\n",
    "            for i in range(len(Y_pred)):\n",
    "                total_loss += L_point(Y_pred[i], positive_clicks[i], negative_clicks[i])\n",
    "            total_loss /= len(Y_pred) # Average over batch\n",
    "\n",
    "\n",
    "            #loss = loss_fn(Y_batch, Y_pred)  # forward-pass\n",
    "            total_loss.backward()  # backward-pass\n",
    "            opt.step()  # update weights\n",
    "\n",
    "            # calculate metrics to show the user\n",
    "            avg_train_loss += total_loss.item() / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        avg_val_loss = 0\n",
    "        model.eval()  # validation mode\n",
    "        with torch.no_grad():  # disable gradient computation\n",
    "            for X_val, annotated_X_val, positive_clicks_val, negative_clicks_val in val_loader:\n",
    "                X_val = X_val.to(device)\n",
    "                positive_clicks_val = [[(y, x) for (y, x) in pos_click] for pos_click in positive_clicks_val]\n",
    "                negative_clicks_val = [[(y, x) for (y, x) in neg_click] for neg_click in negative_clicks_val]\n",
    "\n",
    "                Y_val_pred = model(X_val)\n",
    "\n",
    "                # Compute validation loss\n",
    "                val_total_loss = 0\n",
    "                for i in range(len(Y_val_pred)):\n",
    "                    val_total_loss += L_point(Y_val_pred[i], positive_clicks_val[i], negative_clicks_val[i], alpha=alpha)\n",
    "                avg_val_loss += val_total_loss.item() / len(val_loader)\n",
    "\n",
    "\n",
    "        toc = time()\n",
    "        print(f' - train loss: {avg_train_loss:.4f} - val loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # Adjust learning rate based on validation loss\n",
    "        lr_scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}, saving model.\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save the model checkpoint\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not improve. Patience counter: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered. Training stopped.\")\n",
    "                break\n",
    "\n",
    "        # Show intermediate results\n",
    "         \n",
    "        # Y_hat = torch.sigmoid(model(X_test.to(device))).detach().cpu()\n",
    "        # clear_output(wait=True)\n",
    "        # for k in range(6):\n",
    "        #     plt.subplot(3, 6, k+1)\n",
    "        #     plt.imshow(np.rollaxis(X_test[k].numpy(), 0, 3), cmap='gray')\n",
    "        #     plt.title('Real')\n",
    "        #     plt.axis('off')\n",
    "\n",
    "        #     plt.subplot(3, 6, k+7)\n",
    "        #     plt.imshow(Y_hat[k, 0], cmap='gray')\n",
    "        #     plt.title('Output')\n",
    "        #     plt.axis('off')\n",
    "        #     plt.subplot(3, 6, k+13)\n",
    "        #     plt.imshow(Y_test[k, 0].detach().cpu(), cmap='gray')\n",
    "        #     plt.title('Ground Truth')\n",
    "        #     plt.axis('off')\n",
    "        # plt.suptitle('%d / %d - train loss: %f - val loss: %f' % (epoch+1, epochs, avg_train_loss, avg_val_loss))\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "parameters = 64\n",
    "\n",
    "class UNet2(nn.Module):\n",
    "    def __init__(self, parameter_count = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        def parameters_from_depth(parameters, depth):\n",
    "            return parameters*2*depth\n",
    "        \n",
    "        par0 = parameters_from_depth(parameters, 1)\n",
    "        par1 = parameters_from_depth(parameters, 2)\n",
    "        par2 = parameters_from_depth(parameters, 3)\n",
    "        par3 = parameters_from_depth(parameters, 4)\n",
    "        par4 = parameters_from_depth(parameters, 5)\n",
    "        \n",
    "        \n",
    "        # encoder (downsampling)\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(3, par0, 3, padding=1),\n",
    "            nn.BatchNorm2d(par0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par0, par0, 3,stride=1, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pool0 = nn.Conv2d(par0, par1, 3,stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(par1, par1, 3, padding=1),\n",
    "            nn.BatchNorm2d(par1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par1, par1, 3,stride=1, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.pool1 = nn.Conv2d(par1, par2, 3,stride=2, padding=1)   # 128 -> 64\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(par2, par2, 3, padding=1),\n",
    "            nn.BatchNorm2d(par2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par2, par2, 3,stride=1, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pool2 = nn.Conv2d(par2, par3, 3,stride=2, padding=1)   # 64 -> 32\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(par3, par3, 3, padding=1),\n",
    "            nn.BatchNorm2d(par3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par3, par3, 3,stride=1, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pool3 = nn.Conv2d(par3, par4, 3,stride=2, padding=1)  # 32 -> 16\n",
    "\n",
    "        # bottleneck\n",
    "        self.bottleneck_conv = nn.Sequential(\n",
    "            nn.Conv2d(par4, par4, 3, padding=1),\n",
    "            nn.BatchNorm2d(par4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par4, par4, 3, padding=1),\n",
    "            nn.BatchNorm2d(par4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par4, par4, 3, padding=1),\n",
    "            nn.BatchNorm2d(par4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # decoder (upsampling)\n",
    "        self.upsample0 = nn.ConvTranspose2d(par4, par3, kernel_size=2, stride=2, padding=0) # 16 -> 32\n",
    "        self.dec0 = nn.Sequential(\n",
    "            nn.Conv2d(par3*2, par3, 3, padding=1),\n",
    "            nn.BatchNorm2d(par3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par3, par3, 3, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.upsample1 = nn.ConvTranspose2d(par3, par2, kernel_size=2, stride=2, padding=0)  # 32 -> 64\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(par2*2, par2, 3, padding=1),\n",
    "            nn.BatchNorm2d(par2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par2, par2, 3, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.upsample2 = nn.ConvTranspose2d(par2, par1, kernel_size=2, stride=2, padding=0)  # 64 -> 128\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(par1*2, par1, 3, padding=1),\n",
    "            nn.BatchNorm2d(par1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par1, par1, 3, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.upsample3 = nn.ConvTranspose2d(par1, par0, kernel_size=2, stride=2, padding=0)  # 128 -> 256\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(par0*2, par0, 3, padding=1),\n",
    "            nn.BatchNorm2d(par0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(par0, par0, 3, padding=1),  # 256 -> 128\n",
    "            nn.BatchNorm2d(par0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.final = nn.Conv2d(par0, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x: \", x.shape)\n",
    "        \n",
    "        # encoder\n",
    "        x1 = self.layer0(x)\n",
    "        e0 = self.pool0(x1) # 256 -> 128\n",
    "        # print(\"e0: \", e0.shape)\n",
    "        e1 = self.pool1(self.layer1(e0)) # 128 -> 64\n",
    "        # print(\"e1: \", e1.shape)\n",
    "        \n",
    "        e2 = self.pool2(self.layer2(e1)) # 64 -> 32\n",
    "        # print(\"e2: \", e2.shape)\n",
    "\n",
    "        e3 = self.pool3(self.layer3(e2)) # 32 -> 16\n",
    "        # print(\"e3: \", e3.shape)\n",
    "\n",
    "        # bottleneck\n",
    "        b = self.bottleneck_conv(e3)\n",
    "        # print(\"b: \", b.shape)\n",
    "\n",
    "        # decoder\n",
    "         # decoder\n",
    "        # print(\"d0: \", d0.shape)\n",
    "        d0 = self.upsample0(b)  # Upsample to # 16 -> 32\n",
    "        d0 = torch.cat([d0, e2], dim=1)  # Concatenate with e2 (32x32)\n",
    "        d0 = self.dec0(d0)\n",
    "\n",
    "        d1 = self.upsample1(d0)  # Upsample to # 32 -> 64\n",
    "        d1 = torch.cat([d1, e1], dim=1)  # Concatenate with e1 (64x64)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        d2 = self.upsample2(d1)  # Upsample to # 64 -> 128\n",
    "        d2 = torch.cat([d2, e0], dim=1)  # Concatenate with e0 (128x128)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        \n",
    "        \n",
    "        d3 = self.upsample3(d2)  # Upsample to # 128 -> 256\n",
    "        d3 = torch.cat([d3, x1], dim=1)  # Concatenate with e0 (256x256)\n",
    "        d3 = self.dec3(d3)\n",
    "        d3 = self.final(d3)\n",
    "        return d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import Dice, JaccardIndex, Accuracy, Recall, Specificity\n",
    "\n",
    "#  Dice overlap, Intersection overUnion, Accuracy, Sensitivity, and Specifici\n",
    "\n",
    "def bce_loss(y_real, y_pred):\n",
    "    return torch.mean(y_pred - y_real*y_pred + torch.log(1 + torch.exp(-y_pred)))\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-6  # To avoid division by zero\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "\n",
    "# def dice_coefficient(y_pred, y_true, epsilon=1e-07):\n",
    "#     y_pred_copy = prediction.clone()\n",
    "\n",
    "#     y_pred_copy[prediction_copy < 0] = 0\n",
    "#     y_pred_copy[prediction_copy > 0] = 1\n",
    "\n",
    "#     intersection = abs(torch.sum(y_pred_copy * y_true))\n",
    "#     union = abs(torch.sum(y_pred_copy) + torch.sum(y_true))\n",
    "#     dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "#     return dice\n",
    "\n",
    "# Intersection overUnion\n",
    "\n",
    "def intersection_over_union(y_true, y_pred):\n",
    "    smooth = 1e-6  # To avoid division by zero\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    union = y_true_f.sum() + y_pred_f.sum() - intersection\n",
    "    return intersection / (union + smooth)\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_pred = y_pred.round()  # Convert probabilities to binary\n",
    "    correct = (y_true == y_pred).float()  # Check if predictions are correct\n",
    "    return correct.sum() / correct.numel()\n",
    "\n",
    "# Sensitivity (measures true positives)\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    y_pred = y_pred.round()  # Convert probabilities to binary\n",
    "    true_positives = (y_true * y_pred).sum()\n",
    "    possible_positives = y_true.sum()\n",
    "    return true_positives / (possible_positives + 1e-6)  # Avoid division by zero\n",
    "\n",
    "# Specificity(measures true negatives)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    y_pred = y_pred.round()  # Convert probabilities to binary\n",
    "    true_negatives = ((1 - y_true) * (1 - y_pred)).sum()\n",
    "    possible_negatives = (1 - y_true).sum()\n",
    "    return true_negatives / (possible_negatives + 1e-6)  # Avoid division by zero\n",
    "\n",
    "\n",
    "def evaluate_model_with_metric(model, device, test_loader, metric_fn):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_metric = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X_batch, Y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)  # Move data to the same device as the model (e.g., GPU)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "\n",
    "            # Forward pass: Get model predictions\n",
    "            Y_pred = model(X_batch)\n",
    "\n",
    "            # Calculate the metric for this batch using the provided metric function\n",
    "            metric_value = metric_fn(Y_batch, Y_pred)\n",
    "            total_metric += metric_value.item() * X_batch.size(0)  # Sum the metric over the batch\n",
    "\n",
    "            total_samples += X_batch.size(0)\n",
    "\n",
    "    # Compute average metric value over the entire test set\n",
    "    avg_metric = total_metric / total_samples\n",
    "    #print(f'Final Model Performance - Average Metric: {avg_metric:.4f}')\n",
    "    \n",
    "    return avg_metric\n",
    "\n",
    "\n",
    "\n",
    "def calculate_segmentation_metrics(y_true,y_pred, device):\n",
    "    y_pred = (y_pred > 0.5).long()  # Convert probabilities to binary\n",
    "    y_true = y_true.long() \n",
    "\n",
    "    #dice_score2=Dice(y_pred, y_true)\n",
    "    dice_func = Dice().to(device)\n",
    "    dice_score  = dice_func(y_pred,y_true)\n",
    "    iou_func = JaccardIndex(task=\"binary\").to(device)\n",
    "    iou_score = iou_func(y_pred,y_true)\n",
    "    accuracy_func = Accuracy(task=\"binary\").to(device)\n",
    "    accuracy_score = accuracy_func(y_pred,y_true)\n",
    "    recall_func = Recall(task=\"binary\").to(device)\n",
    "    sensitivity_score=recall_func(y_pred,y_true)\n",
    "    specificity_func = Specificity(task=\"binary\").to(device)\n",
    "    specificity_score=specificity_func(y_pred,y_true)\n",
    "    print (dice_score)\n",
    "    print(iou_score)\n",
    "    print(accuracy_score)\n",
    "    print(sensitivity_score)\n",
    "    print(specificity_score)\n",
    "    metrics = {\n",
    "            'Dice': dice_score,\n",
    "            'IoU': iou_score,\n",
    "            'Accuracy': accuracy_score,\n",
    "            'Sensitivity': sensitivity_score,\n",
    "            'Specificity': specificity_score\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists to store metrics for each batch\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    accuracy_scores = []\n",
    "    sensitivity_scores = []\n",
    "    specificity_scores = []\n",
    "    \n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Assuming each batch has images and masks\n",
    "            images, masks = batch\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # Forward pass to get predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert outputs to probabilities if necessary\n",
    "            y_pred = torch.sigmoid(outputs)  # Assuming binary segmentation\n",
    "            \n",
    "            # Calculate metrics for this batch\n",
    "            metrics = calculate_segmentation_metrics(masks, y_pred, device)\n",
    "            \n",
    "            # Append each metric\n",
    "            dice_scores.append(metrics['Dice'].item())\n",
    "            iou_scores.append(metrics['IoU'].item())\n",
    "            accuracy_scores.append(metrics['Accuracy'].item())\n",
    "            sensitivity_scores.append(metrics['Sensitivity'].item())\n",
    "            specificity_scores.append(metrics['Specificity'].item())\n",
    "    \n",
    "    # Compute average for each metric\n",
    "    avg_metrics = {\n",
    "        'Dice': sum(dice_scores) / len(dice_scores),\n",
    "        'IoU': sum(iou_scores) / len(iou_scores),\n",
    "        'Accuracy': sum(accuracy_scores) / len(accuracy_scores),\n",
    "        'Sensitivity': sum(sensitivity_scores) / len(sensitivity_scores),\n",
    "        'Specificity': sum(specificity_scores) / len(specificity_scores)\n",
    "    }\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "#loss function for abalation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training samples: 140\n",
      "Validation samples: 30\n",
      "Testing samples: 30\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 256, 256]           3,584\n",
      "       BatchNorm2d-2        [-1, 128, 256, 256]             256\n",
      "              ReLU-3        [-1, 128, 256, 256]               0\n",
      "            Conv2d-4        [-1, 128, 256, 256]         147,584\n",
      "       BatchNorm2d-5        [-1, 128, 256, 256]             256\n",
      "              ReLU-6        [-1, 128, 256, 256]               0\n",
      "            Conv2d-7        [-1, 256, 128, 128]         295,168\n",
      "            Conv2d-8        [-1, 256, 128, 128]         590,080\n",
      "       BatchNorm2d-9        [-1, 256, 128, 128]             512\n",
      "             ReLU-10        [-1, 256, 128, 128]               0\n",
      "           Conv2d-11        [-1, 256, 128, 128]         590,080\n",
      "      BatchNorm2d-12        [-1, 256, 128, 128]             512\n",
      "             ReLU-13        [-1, 256, 128, 128]               0\n",
      "           Conv2d-14          [-1, 384, 64, 64]         885,120\n",
      "           Conv2d-15          [-1, 384, 64, 64]       1,327,488\n",
      "      BatchNorm2d-16          [-1, 384, 64, 64]             768\n",
      "             ReLU-17          [-1, 384, 64, 64]               0\n",
      "           Conv2d-18          [-1, 384, 64, 64]       1,327,488\n",
      "      BatchNorm2d-19          [-1, 384, 64, 64]             768\n",
      "             ReLU-20          [-1, 384, 64, 64]               0\n",
      "           Conv2d-21          [-1, 512, 32, 32]       1,769,984\n",
      "           Conv2d-22          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-23          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-24          [-1, 512, 32, 32]               0\n",
      "           Conv2d-25          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-26          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-27          [-1, 512, 32, 32]               0\n",
      "           Conv2d-28          [-1, 640, 16, 16]       2,949,760\n",
      "           Conv2d-29          [-1, 640, 16, 16]       3,687,040\n",
      "      BatchNorm2d-30          [-1, 640, 16, 16]           1,280\n",
      "             ReLU-31          [-1, 640, 16, 16]               0\n",
      "           Conv2d-32          [-1, 640, 16, 16]       3,687,040\n",
      "      BatchNorm2d-33          [-1, 640, 16, 16]           1,280\n",
      "             ReLU-34          [-1, 640, 16, 16]               0\n",
      "           Conv2d-35          [-1, 640, 16, 16]       3,687,040\n",
      "      BatchNorm2d-36          [-1, 640, 16, 16]           1,280\n",
      "             ReLU-37          [-1, 640, 16, 16]               0\n",
      "  ConvTranspose2d-38          [-1, 512, 32, 32]       1,311,232\n",
      "           Conv2d-39          [-1, 512, 32, 32]       4,719,104\n",
      "      BatchNorm2d-40          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-41          [-1, 512, 32, 32]               0\n",
      "           Conv2d-42          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-43          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-44          [-1, 512, 32, 32]               0\n",
      "  ConvTranspose2d-45          [-1, 384, 64, 64]         786,816\n",
      "           Conv2d-46          [-1, 384, 64, 64]       2,654,592\n",
      "      BatchNorm2d-47          [-1, 384, 64, 64]             768\n",
      "             ReLU-48          [-1, 384, 64, 64]               0\n",
      "           Conv2d-49          [-1, 384, 64, 64]       1,327,488\n",
      "      BatchNorm2d-50          [-1, 384, 64, 64]             768\n",
      "             ReLU-51          [-1, 384, 64, 64]               0\n",
      "  ConvTranspose2d-52        [-1, 256, 128, 128]         393,472\n",
      "           Conv2d-53        [-1, 256, 128, 128]       1,179,904\n",
      "      BatchNorm2d-54        [-1, 256, 128, 128]             512\n",
      "             ReLU-55        [-1, 256, 128, 128]               0\n",
      "           Conv2d-56        [-1, 256, 128, 128]         590,080\n",
      "      BatchNorm2d-57        [-1, 256, 128, 128]             512\n",
      "             ReLU-58        [-1, 256, 128, 128]               0\n",
      "  ConvTranspose2d-59        [-1, 128, 256, 256]         131,200\n",
      "           Conv2d-60        [-1, 128, 256, 256]         295,040\n",
      "      BatchNorm2d-61        [-1, 128, 256, 256]             256\n",
      "             ReLU-62        [-1, 128, 256, 256]               0\n",
      "           Conv2d-63        [-1, 128, 256, 256]         147,584\n",
      "      BatchNorm2d-64        [-1, 128, 256, 256]             256\n",
      "             ReLU-65        [-1, 128, 256, 256]               0\n",
      "           Conv2d-66          [-1, 1, 256, 256]           1,153\n",
      "================================================================\n",
      "Total params: 41,578,625\n",
      "Trainable params: 41,578,625\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 1517.00\n",
      "Params size (MB): 158.61\n",
      "Estimated Total Size (MB): 1676.36\n",
      "----------------------------------------------------------------\n",
      "* Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/0f/0/213641/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet2()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m summary(model, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrain_with_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#do evaluate model performace on loss functios\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#avg_dice = evaluate_model_with_metric(model, device, test_loader, dice_coefficient)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# specificity_ = evaluate_model_with_metric(model, device, test_loader, specificity)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# print(f'Final Model Performance - Specificity Metric: {specificity_:.4f}')\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Model Performance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [35], line 20\u001b[0m, in \u001b[0;36mtrain_with_validation\u001b[0;34m(device, model, opt, loss_fn, epochs, train_loader, val_loader, test_loader, patience)\u001b[0m\n\u001b[1;32m     18\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# train mode\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, annotated_X, positive_clicks, negative_clicks \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     21\u001b[0m     X_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m     annotated_X \u001b[38;5;241m=\u001b[39m annotated_X\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn [27], line 83\u001b[0m, in \u001b[0;36mSkinLesionLoader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     80\u001b[0m mask_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Y) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Generate positive and negative clicks using the mask\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m positive_clicks, negative_clicks \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_clicks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_positives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmargin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Draw clicks on the original image\u001b[39;00m\n\u001b[1;32m     86\u001b[0m annotated_image \u001b[38;5;241m=\u001b[39m draw_clicks_on_original(image_path, positive_clicks, negative_clicks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircle_radius, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mborder_thickness)\n",
      "Cell \u001b[0;32mIn [1], line 25\u001b[0m, in \u001b[0;36mgenerate_clicks\u001b[0;34m(mask, num_positives, num_negatives, margin, var)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(positive_clicks) \u001b[38;5;241m<\u001b[39m num_positives:\n\u001b[1;32m     24\u001b[0m     new_pos_click \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(object_pixels)\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_dispersed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pos_click\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_clicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     26\u001b[0m         positive_clicks\u001b[38;5;241m.\u001b[39mappend(new_pos_click)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Sample negative clicks from background pixels\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [1], line 17\u001b[0m, in \u001b[0;36mgenerate_clicks.<locals>.is_dispersed\u001b[0;34m(new_point, existing_points, min_distance)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_dispersed\u001b[39m(new_point, existing_points, min_distance):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m existing_points:\n\u001b[0;32m---> 17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_point\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m min_distance:\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/site-packages/numpy/linalg/linalg.py:2513\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_norm_dispatcher)\n\u001b[1;32m   2363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm\u001b[39m(x, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m \u001b[38;5;124;03m    Matrix or vector norm.\u001b[39;00m\n\u001b[1;32m   2366\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \n\u001b[1;32m   2512\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2513\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (inexact, object_)):\n\u001b[1;32m   2516\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/site-packages/numpy/core/_asarray.py:102\u001b[0m, in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m like \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_like(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, like\u001b[38;5;241m=\u001b[39mlike)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/zhome/44/9/212447/venv_1/bin/python3\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "# pip install torchsummary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "data_path = \"/dtu/datasets1/02516//PH2_Dataset_images\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),    # Resize to a fixed size\n",
    "    transforms.ToTensor()             # Convert to tensor\n",
    "])\n",
    "# Create dataset instances for train, validation, and test\n",
    "train_dataset = SkinLesionLoader(transform=transform, dataset_path=data_path, split='train')\n",
    "val_dataset = SkinLesionLoader(transform=transform, dataset_path=data_path, split='val')\n",
    "test_dataset = SkinLesionLoader(transform=transform, dataset_path=data_path, split='test')\n",
    "\n",
    "#Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "model = UNet2().to(device)\n",
    "summary(model, (3, 256, 256))\n",
    "\n",
    "train_with_validation(device, model, optim.Adam(model.parameters()), L_point, 20, train_loader, val_loader, test_loader)\n",
    "\n",
    "#do evaluate model performace on loss functios\n",
    "\n",
    "#avg_dice = evaluate_model_with_metric(model, device, test_loader, dice_coefficient)\n",
    "# print(f'Final Model Performance - Dice Coefficient Metric: {avg_dice:.4f}')\n",
    "# intersect = evaluate_model_with_metric(model, device, test_loader, intersection_over_union)\n",
    "# print(f'Final Model Performance - Intersection Over Union Metric: {intersect:.4f}')\n",
    "# accuracy_ = evaluate_model_with_metric(model, device, test_loader, accuracy)\n",
    "# print(f'Final Model Performance - Accuracy Metric: {accuracy_:.4f}')\n",
    "# sensitivity_ = evaluate_model_with_metric(model, device, test_loader, sensitivity)\n",
    "# print(f'Final Model Performance - Sensitivity Metric: {sensitivity_:.4f}')\n",
    "# specificity_ = evaluate_model_with_metric(model, device, test_loader, specificity)\n",
    "# print(f'Final Model Performance - Specificity Metric: {specificity_:.4f}')\n",
    "\n",
    "print(\"Final Model Performance\")\n",
    "metrics = evaluate_model(model, test_loader, device)\n",
    "for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value:.4f}\")  # Format to 4 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
